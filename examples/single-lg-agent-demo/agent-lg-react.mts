// agent.mts

// å¯¼å…¥ dotenv é…ç½®
import { config } from 'dotenv';
import { resolve } from 'path';

// ä»å½“å‰ç›®å½•åŠ è½½ config.env æ–‡ä»¶
config({ path: resolve(process.cwd(), './config.env') });

import { TavilySearch } from "@langchain/tavily";
import { ChatOpenAI } from "@langchain/openai";
import { MemorySaver } from "@langchain/langgraph";
import { HumanMessage } from "@langchain/core/messages";
import { createReactAgent } from "@langchain/langgraph/prebuilt";

// Define the tools for the agent to use
const agentTools = [new TavilySearch({ maxResults: 3 })];  // INFO è¿™é‡Œå…³äº Tool çš„è®¾è®¡éƒ½æ˜¯å¾ˆç®€å•çš„ list å…¨é‡æ–¹å¼ã€‚
const agentModel = new ChatOpenAI({ 
  model: "deepseek-chat",
  temperature: 0,
  configuration: {
    baseURL: process.env.OPENAI_BASE_URL,
    apiKey: process.env.OPENAI_API_KEY,
  }
});

// Initialize memory to persist state between graph runs
const agentCheckpointer = new MemorySaver(); // TODO è¿™é‡Œçš„è®°å¿†åº•å±‚æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ
const agent = createReactAgent({
  llm: agentModel,
  tools: agentTools,
  checkpointSaver: agentCheckpointer,
});

// Now it's time to use!
const agentFinalState = await agent.invoke(
  { messages: [new HumanMessage("ä»Šå¤©æœ‰ä»€ä¹ˆçƒ­ç‚¹æ–°é—»å—ï¼Ÿ")] },
  { configurable: { thread_id: "42" } },
);

console.log(
    'ğŸ˜º',
     agentFinalState.messages[agentFinalState.messages.length - 1].content,
);

const agentNextState = await agent.invoke(
  { messages: [new HumanMessage("what about ny")] },
  { configurable: { thread_id: "42" } },
);

console.log(
    'ğŸ˜º',
    agentNextState.messages[agentNextState.messages.length - 1].content,
);